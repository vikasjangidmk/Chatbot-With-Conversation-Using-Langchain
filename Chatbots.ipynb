{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  ## Adding all the enviroment variables\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000246F961D450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000246F961EC10>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krish, it's nice to meet you!\\n\\nThat's quite a title! As a Chief AI Engineer, I imagine you're involved in some exciting projects. \\n\\nWhat are you currently working on that you're most passionate about?  I'm always eager to learn more about the applications of AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 22, 'total_tokens': 92, 'completion_time': 0.127272727, 'prompt_time': 0.00014072, 'queue_time': 0.014011707, 'total_time': 0.127413447}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e828dea5-7ee8-47d9-9074-0d3c31b50a4d-0', usage_metadata={'input_tokens': 22, 'output_tokens': 70, 'total_tokens': 92})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Krish and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Krish, and you are a GenAI Engineer!  😊  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm happy to chat.   \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 90, 'total_tokens': 135, 'completion_time': 0.081818182, 'prompt_time': 0.003777811, 'queue_time': 0.010680169, 'total_time': 0.085595993}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b66deecf-b7ab-44a0-8d20-64a8a1f72ef7-0', usage_metadata={'input_tokens': 90, 'output_tokens': 45, 'total_tokens': 135})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core .messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Krish And I am a GenAI Engineer\"),\n",
    "        AIMessage(content=\"Hello Krish! It's a pleasure to meet you.  \\n\\nThat's a fascinating role. What kind of AI projects are you working on these days? I'm always eager to learn about the latest developments in the field.\"),\n",
    "        HumanMessage(content=\"Hey Whats my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Message History**\n",
    "- We can use a message History class to wrap our model it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"Hi, My name is Krish And I am a GenAI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's awesome, Krish! It's great to meet you.  \\n\\nAs a fellow AI, I'm always interested in connecting with other people who are working in this exciting field. What kind of work do you do as a GenAI Engineer?  \\n\\nI'm eager to learn more about your experience and what you find most interesting about generative AI.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Krish. 😊  \\n\\nI remember from our introduction!  How can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 114, 'total_tokens': 139, 'completion_time': 0.045454545, 'prompt_time': 0.00391461, 'queue_time': 0.009551219, 'total_time': 0.049369155}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7dae77ba-260f-4203-8314-7b077572473d-0', usage_metadata={'input_tokens': 114, 'output_tokens': 25, 'total_tokens': 139})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(\"What's my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the config --> Session Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to know!\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what's my name?\")],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello John! It's nice to meet you. 😊 \\n\\nWhat can I do for you today?\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My Name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John, as you told me a moment ago.  😄 \\n\\nIs there anything else I can help you with? \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what's my name\")],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt Templates and Message Chat History Using Langchian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# What are Prompt Templates in LangChain?\n",
    "\n",
    "Prompt Templates are structured ways to define the input format for language models in LangChain. \n",
    "They allow you to dynamically insert variables into predefined templates, which helps in creating \n",
    "reusable, flexible, and consistent prompts for various tasks such as translation, summarization, \n",
    "question answering, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of Prompt Templates\n",
    "\n",
    "1. **Dynamic Variables**:\n",
    "   Use placeholders (e.g., `{variable_name}`) to insert dynamic values.\n",
    "\n",
    "2. **Reusable**:\n",
    "   Templates can be reused across different tasks.\n",
    "\n",
    "3. **Consistent**:\n",
    "   Enforces uniformity in prompt formats, reducing errors.\n",
    "\n",
    "4. **Supports Complex Tasks**:\n",
    "   Helps handle multi-turn conversations or nested instructions.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all the question of the nest of your ability.\",\n",
    "            ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Vikas, it's nice to meet you! 👋\\n\\nI'm happy to help with any questions you have. Just ask away! 😊 \\n\\nWhat can I do for you today?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 32, 'total_tokens': 78, 'completion_time': 0.083636364, 'prompt_time': 0.00030618, 'queue_time': 0.014905940000000001, 'total_time': 0.083942544}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-48689375-3a92-4046-8812-dd9f7b307a53-0', usage_metadata={'input_tokens': 32, 'output_tokens': 46, 'total_tokens': 78})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Vikas\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Vikas\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Vikas,\\n\\nIt's nice to meet you!  \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 32, 'total_tokens': 72, 'completion_time': 0.072727273, 'prompt_time': 0.000304709, 'queue_time': 0.01399493, 'total_time': 0.073031982}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a8bd1ba4-e7f3-43a4-9897-2c27672fc13c-0', usage_metadata={'input_tokens': 32, 'output_tokens': 40, 'total_tokens': 72})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all the question of the nest of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते विकास! \\n\\nमैं आपकी मदद करने के लिए यहाँ हूँ। आपके कोई सवाल हैं तो बेझिझक पूछिए, मैं जो भी कर सकता हूँ, उसका प्रयास करूँगा। 😊 \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Vikas\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Now wrap this more complicated chain in a message History class. This time because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते विकास! \\n\\nमुझे खुशी है तुम्हारी मुलाक़ात हुई।  \\n\\nआप क्या पूछना चाहते हैं? 😊 \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, I am Vikas\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तुम्हारा नाम विकास है।  😊  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, What is my name?\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Managing the Conversation History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, If is important to add a step that limits the size of the messages you are passing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# What is `trim_message` in LangChain?\n",
    "\n",
    "The `trim_message` function in LangChain is used to clean and shorten a message. This is particularly useful when dealing with verbose outputs or messages from language models that need to be processed or displayed in a concise format.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of `trim_message`\n",
    "\n",
    "1. **Removes Redundancies**:\n",
    "   Trims excessive details or unnecessary information from the message.\n",
    "\n",
    "2. **Length Management**:\n",
    "   Ensures the message stays within a defined character limit, making it suitable for use in UIs or logs.\n",
    "\n",
    "3. **Enhances Readability**:\n",
    "   Keeps the message focused on its core content by removing extraneous details.\n",
    "\n",
    "4. **Customizable**:\n",
    "   Supports configurable trimming logic to meet specific requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "Here’s a basic example of how `trim_message` might be used:\n",
    "\n",
    "```python\n",
    "from langchain_utils import trim_message\n",
    "\n",
    "# Example message\n",
    "long_message = \"This is a very long message that contains a lot of unnecessary details and information that can be trimmed for better readability.\"\n",
    "\n",
    "# Trim the message to a maximum of 50 characters\n",
    "trimmed_message = trim_message(long_message, max_length=50)\n",
    "\n",
    "print(\"Trimmed Message:\", trimmed_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Generative AI\\Chatbot-With-Conversation-Using-Langchain\\Generative4\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Generative AI\\Chatbot-With-Conversation-Using-Langchain\\Generative4\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vikas\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to your personal information, including your ice cream preferences.  \\n\\nWhat's your favorite flavor?  🍦😋\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What\\'s 2 + 2?\". 😊 \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did i ask\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's wrap this in the message history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any personal information about you, including your name.  \\n\\nWould you like to tell me your name? 😊  \\n\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= with_message_history.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What's my name\")],\n",
    "    \"language\":\"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to your personal preferences, like your favorite ice cream flavor.  \\n\\nWhat's your favorite ice cream flavor?  🍦😊\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
